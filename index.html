
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">

<html>
<head>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">





<!--- for clickable headers -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>
    <style type="text/css">th, td { padding: 17px;} </style>
<!--- for clickable headers -->

<script type="text/javascript" src="js/hidebib.js"></script>


<style type="text/css">
  a {
  color: #1772d0;
  text-decoration:none;
  }
  a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
  }
  body,td,th {
        font-family: 'Lato', Verdana, Helvetica, sans-serif;
        font-size: 14px
  }
  strong {
        font-family: 'Lato', Verdana, Helvetica, sans-serif;
        font-size: 14px
  }
  strongred {
        font-family: 'Lato', Verdana, Helvetica, sans-serif;
        color: 'red';
        font-size: 14px
  }
  heading {
        font-family: 'Lato', Verdana, Helvetica, sans-serif;
        font-size: 15px;
        font-weight: 700
  }
  #news {
            overflow-y: scroll;
            height: 177px;
            margin-top: 0px;
            padding-top: 0px;
        }
  #award {
            overflow-y: scroll;
            height: 177px;
            margin-top: 0px;
            padding-top: 0px;
        }

  #service {
            overflow-y: scroll;
            height: 100px;
            margin-top: 0px;
            padding-top: 0px;
        }


</style>


<link rel="icon" type="image/png" href="images/hao1-c.png">





<title>Hao Zhou</title>
<meta name="Hao Zhou's Homepage"http-equiv="Content-Type" content="Hao Zhou's Homepage">

<link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
<script src='https://www.google.com/recaptcha/api.js'></script>

<!--
<script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]//function(){
    (i[r].q=i[r].q//[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-41102261-2', 'auto');
    ga('send', 'pageview');
</script>
-->


<!---

<style>
#return_top {
  display: none;
  
  bottom: 20px;
  right: 30px;
  z-index: 99;
  font-size: 18px;
  border: none;
  outline: none;
  background-color: #336BFF;
  color: white;
  cursor: pointer;
  padding: 15px;
  border-radius: 4px;
}

#return_top:hover {
  background-color: #555;
}
</style>

<button onclick="topFunction()" id="return_top" title="Return to top">Return to top</button>

<script>
//Get the button
var mybutton = document.getElementById("return_top");
// When the user scrolls down 20px from the top of the document, show the button
window.onscroll = function() {scrollFunction()};
function scrollFunction() {
  if (document.body.scrollTop > 20 || document.documentElement.scrollTop > 20) {
    mybutton.style.display = "block";
  } else {
    mybutton.style.display = "none";
  }
}
// When the user clicks on the button, scroll to the top of the document
function topFunction() {
  document.body.scrollTop = 0;
  document.documentElement.scrollTop = 0;
}
</script>
--->


</head>


<body>
  







<!--bio-->


  <!-- <table width="1080" border="0" align="center" border="0" cellspacing="0" cellpadding="20"> -->
    <table style="width:100%;max-width:1080px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            
            <tr>
            <td width="67%" valign="middle" align="justify">
              <p align="center"><font size="6px">Hao Zhou 周浩</font>
                <br>hao *DOT* zhou *AT* psu *DOT* edu</p>
              <p>
                I am currently in my third year of Ph.D. studies at PennState, under the guidance of <a href="https://www.cse.psu.edu/~mkg31/">Prof. Mahanth Gowda</a>. I was also fortunate to receive mentorship from <a href="https://people.cs.umass.edu/~jxiong/">Prof. Jie Xiong</a> during my internships at Microsoft Research Asia. 
              </p>

              <p>
                <strong>My main research spans across wearable/wireless sensing with a machine learning focus in the fields of pose estimation, digital health, and accessibility topics such as sign language recognition.</strong> 
              </p>  
              

              <br>

              <!-- <p style="text-align:center"> "Brevity is the soul of wit" -- William Shakespeare  </p> -->

              <p style="text-align:left"> <FONT COLOR="#ff0000">I am open to collabration on wearable and wireless sensing with machine learning focus, please shot me an email and we will work things out from there. </FONT> </p>
        
              <br>
              <p style="text-align:center">
                <a href="cv/cv.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=Oc7AeVYAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/hzhou3">Github</a> &nbsp/&nbsp
                <a href="https://twitter.com/hao_zhh">X</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/hao-zhou-37953316b">LinkedIn</a> 
              </p>
            </td>



            <td width="30%"valign="top"><img src="images/hao.JPG" width="100%"></td>
            
          </tr>

        </table>







<!--navigator-->
      
        <table width="100%" align="right" border="0" cellspacing="0" cellpadding="20" style="border-collapse: collapse; margin-top:10px">
          <!-- <p style="text-align:right"> "Brevity is the soul of wit" -- William Shakespeare  </p> -->
          <tr style="border-bottom: 2px solid rgb(192, 192, 192)">
          <td width="100%" valign="top" align="middle">
            <a href="#news"><big><b>News</b></big></a> &emsp;<span>&#183;</span>&emsp;
            <a href="#publications"><big><b>Publications</b></big></a> &emsp;<span>&#183;</span>&emsp;
            <a href="#service"><big><b>Services</b></big></a> &emsp;<span>&#183;</span>&emsp;
            <a href="#award"><big><b>Awards & Honors</b></big></a>
            </td>
          </td>
          </td>
          </tr>
        </table>







<!--News-->


      
    <button style="border:0px transparent; background-color: transparent;outline:none;"type="button" class="collapsible" data-toggle="collapse" data-target="#news" id="highlights_button"><h3>News</h3></button>

    <div id="news" class="collapse in">

    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr><td>
                    <ul>
                       <span> [2024-05] I will be interning at Ditial Health team at Samsung Research American.</span><br>
                       <span> [2024-03] ASLRing is accepted IoTDI 2024.</span><br>
                       <span> [2023-11] I passed the comprehensive exam!</span><br>
                       <span> [2023-11] Received Penn State International Travel Grant.</span><br>
                       <span> [2023-09] Received Student Travel Grant from MobiCom'23.</span><br>
                       <span> [2023-08] SignQuery is accepted to MobiCom'23.</span><br>
                       <span> [2023-05] OmniRing won <strong>Best Paper Award for Edge IoT AI</strong>.</span><br>
                       <span> [2023-04] I will be an intern at Microsoft Research Asia!</span><br>
                       <span> [2023-01] OmniRing is accepted to IoTDI 2023.</span><br>
		                   <span> [2022-09] Received an outstanding TA award! Thanks everyone!</span><br>
                       <span> [2022-04] ssLOTR is accepted to IMWUT 2022.</span><br>
                       <span> [2021-10] DACHash won the <strong>Best Paper Award</strong>.</span><br>
                       <span> [2021-08] DACHash is accepted to SBAC-PAD 2021.</span><br>
                       <span> [2021-07] I started Ph.D. at PennState.</span><br>
                       <span> [2020-08] My first paper is accepted to AI4I 2020.</span> <br>     
                    </ul>
		    </td></tr>
      </table>
        </div>

  <hr class="soft">





<!--Publications-->


    <button style="border:0px transparent; background-color: transparent;outline:none;"type="button" class="collapsible" data-toggle="collapse" data-target="#publications" id="publications_button"><h3>Publications</h3></button>
    <div id="publications" class="collapse in">

    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
      <tr><td>
      </td></tr>
    </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">














          <tr>
            <td width="33%" valign="top"><a href="pubs/aslring.png"><img src="pubs/aslring.png" width="100%" style="border-style: none"></a>
            <td width="67%" valign="top">
              <p>
              <a href="" id="">
              <heading>ASLRing: American Sign Language Recognition with Meta-Learning on Wearables</heading></a><br>
              <strong>Hao Zhou</strong>, <FONT COLOR="#777777">Taiting Lu, Kenneth DeHaan, Mahanth Gowda</FONT>
              <br>
              <em>IEEE/ACM IoTDI</em>, 2024.
              <br>
              </p>

              <div class="paper" id="aslring">
                <a href="javascript:toggleblock('aslring_abs')">abstract</a> / 
                <a href="https://www.cse.psu.edu/∼mkg31/projects/aslring/">project page</a> / 
                <a href="bib/aslring.txt">bibtex</a> 
                <!-- <a href="slides/aslring.pdf">slides</a>  -->
                 
                <p align="justify"> <i id="aslring_abs">Sign Language is widely used by over 500 million Deaf and hard of hearing (DHH) individuals in their daily lives. While prior works made notable efforts to show the feasibility of recognizing signs with various sensing modalities both from the wireless and wearable domains, they recruited sign language learners for validation. Based on our interactions with native sign language users, we found that signal diversity hinders the generalization of users (e.g., users from different backgrounds interpret signs differently, and native users have complex articulated signs), thus resulting in recognition difficulty. While multiple solutions (e.g., increasing diversity of data, harvesting virtual data from sign videos) are possible, we propose ASLRing that addresses the sign language recognition problem from a meta-learning perspective by learning an inherent knowledge about diverse spaces of signs for fast adaptation. ASLRing bypasses expensive data collection process and avoids the limitation of leveraging virtual data from sign videos (e.g., occlusions, overexposure, low-resolution). To validate ASLRing, instead of recruiting learners, we conducted a comprehensive user study with a database with 1080 sentences generated by a vocabulary size of 1057 from 14 native sign language users and achieved a 26.9% word error rate, and we also validated ASLRing in diverse settings.</i></p>
                </div>
            </td>
        </tr>











          <tr>
            <td width="33%" valign="top"><a href="pubs/signquery.png"><img src="pubs/signquery.png" width="100%" style="border-style: none"></a>
            <td width="67%" valign="top">
              <p>
              <a href="https://dl.acm.org/doi/abs/10.1145/3570361.3613286" id="">
              <heading>SignQuery: A Natural User Interface and Search Engine for Sign Languages with Wearable Sensors</heading></a><br>
              <strong>Hao Zhou</strong>, <FONT COLOR="#777777">Taiting Lu, Kristina McKinnie, Joseph Palagano, Kenneth DeHaan, Mahanth Gowda</FONT>
              <br>
              <em>ACM MobiCom</em>, 2023.
              <br>
              </p>

              <div class="paper" id="signquery">
                <!-- <a href="https://www.cse.psu.edu/~mkg31/projects/omniring/">project page</a> / -->
                <a href="javascript:toggleblock('signquery_abs')">abstract</a> / 
                <a href="bib/signquery.txt">bibtex</a> /
                <a href="slides/signquery.pdf">slides</a> 
                <!-- <a href="https://raw.githubusercontent.com/hzhou3/hzhou3.github.io/main/bib/omniring.bib">bibtex</a> -->
                 
                <p align="justify"> <i id="signquery_abs">Search Engines such as Google, Baidu, and Bing have revolutionized the way we interact with the cyber world with a number of applications in recommendations, learning, advertisements, healthcare, entertainment, etc. In this paper, we design search engines for sign languages such as American Sign Language (ASL). Sign languages use hand and body motion for communication with rich grammar, complexity, and vocabulary that is comparable to spoken languages. This is the primary language for the Deaf community with a global population of ≈ 500 million. However, search engines that support sign language queries in native form do not exist currently. While translating a sign language to a spoken language and using existing search engines might be one possibility, this can miss critical information because existing translation systems are either limited in vocabulary or constrained to a specific domain. In contrast, this paper presents a holistic approach where ASL queries in native form as well as ASL videos and textual information available online are converted into a common representation space. Such a joint representation space provides a common framework for precisely representing different sources of information and accurately matching a query with relevant information that is available online. Our system uses low-intrusive wearable sensors for capturing the sign query. To minimize the training overhead, we obtain synthetic training data from a large corpus of online ASL videos across diverse topics. Evaluated over a set of Deaf users with native ASL fluency, the accuracy is comparable with state-of-the-art recommendation systems for Amazon, Netflix, Yelp, etc., suggesting the usability of the system in the real world. For example, the recall@10 of our system is 64.3%, i.e., among the top ten search results, six of them are relevant to the search query. Moreover, the system is robust to variations in signing patterns, dialects, sensor positions, etc.</i></p>
                </div>
            </td>
        </tr>















          <tr>
            <td width="33%" valign="top"><a href="pubs/earface.png"><img src="pubs/earface.png" width="100%" style="border-style: none"></a>
            <td width="67%" valign="top">
              <p>
              <a href="https://dl.acm.org/doi/10.1145/3614438" id="">
              <heading>I am an Earphone and I can Hear my Users Face: Facial Landmark Tracking using Smart Earphones</heading></a><br>
               <FONT COLOR="#777777">Shijia Zhang, Taiting Lu,</FONT> <strong>Hao Zhou</strong><FONT COLOR="#777777">, Yilin Liu, Runze Liu, Mahanth Gowda</FONT>
              <br>
              <em>ACM Transactions on Internet of Things</em>, 2023.
              <br>
              </p>
              <div class="paper" id="earface">
                <a href="javascript:toggleblock('earface_abs')">abstract</a> / 
                <a href="bib/earface.txt">bibtex</a>

                <p align="justify"> <i id="earface_abs">This paper presents EARFace, a system that shows the feasibility of tracking facial landmarks for 3D facial reconstruction using in-ear acoustic sensors embedded within smart earphones. This enables a number of applications in the areas of facial expression tracking, user-interfaces, AR/VR applications, affective computing, accessibility, etc. While conventional vision-based solutions break down under poor lighting, occlusions, and also suffer from privacy concerns, earphone platforms are robust to ambient conditions, while being privacy-preserving. In contrast to prior work on earable platforms that perform outer-ear sensing for facial motion tracking, EARFace shows the feasibility of completely in-ear sensing with a natural earphone form-factor, thus enhancing the comfort levels of wearing. The core intuition exploited by EARFace is that the shape of the ear canal changes due to the movement of facial muscles during facial motion. EARFace tracks the changes in shape of the ear canal by measuring ultrasonic channel frequency response (CFR) of the inner ear, ultimately resulting in tracking of the facial motion. A transformer based machine learning (ML) model is designed to exploit spectral and temporal relationships in the ultrasonic CFR data to predict the facial landmarks of the user with an accuracy of 1.83 mm. Using these predicted landmarks, a 3D graphical model of the face that replicates the precise facial motion of the user is then reconstructed. Domain adaptation is further performed by adapting the weights of layers using a group-wise and differential learning rate. This decreases the training overhead in EARFace. The transformer based ML model runs on smartphone devices with a processing latency of 13 ms and an overall low power consumption profile. Finally, usability studies indicate higher levels of comforts of wearing EARFace’s earphone platform in comparison with alternative form-factors.</i></p>
                </div>
            </td>
        </tr>













 
          <tr>
            <td width="33%" valign="top"><a href="pubs/omniring.png"><img src="pubs/omniring.png" width="100%" style="border-style: none"></a>
            <td width="67%" valign="top">
              <p>
              <a href="https://dl.acm.org/doi/abs/10.1145/3576842.3582382" id="">
              <heading>One Ring to Rule Them All: An Open Source Smartring Platform for Finger Motion Analytics and Healthcare Applications</heading></a><br>
              <strong>Hao Zhou</strong>, <FONT COLOR="#777777">Taiting Lu, Yilin Liu, Shijia Zhang, Runze Liu, Mahanth Gowda</FONT>
              <br>
              <em>IEEE/ACM IoTDI</em>, 2023. <strong style="color:red">(Best Paper Award for Edge IoT AI)</strong>
              <br>
              </p>

              <div class="paper" id="omniring">
                <a href="javascript:toggleblock('omniring_abs')">abstract</a> /
                <a href="https://www.cse.psu.edu/~mkg31/projects/omniring/">project page</a> / 
                <a href="bib/omniring.txt">bibtex</a>

                <p align="justify"> <i id="omniring_abs">This paper presents OmniRing, an open-source smartring platform with IMU and PPG sensors for activity tracking and health analytics applications. Smartring platforms are on the rise because of comfortable wearing, with the market size expected to reach $92 million soon. Nevertheless, most existing platforms are either commercial and proprietary without details of software/hardware or use suboptimal PCB design resulting in bulky form factors, inconvenient for wearing in daily life. Towards bridging the gap, OmniRing presents an extensible design of a smartring with a miniature form factor, longer battery life, wireless communication, and water resistance so that users can wear it all the time. Towards this end, OmniRing exploits opportunities in SoC, and carefully integrates the sensing units with a microcontroller and BLE modules. The electronic components are integrated on both sides of a flexible PCB that is bent in the shape of a ring and enclosed in a flexible and waterproof case for smooth skin contact. The overall cost is under $25, with weight of 2.5g, and up to a week of battery life. Extensive usability surveys validate the comfort levels. To validate the sensing capabilities, we enable an application in 3D finger motion tracking. By extracting synthetic training data from public videos coupled with data augmentation to minimize the overhead of training data generation for a new platform, OmniRing designs a transformer-based model that exploits correlations across fingers and time to track 3D finger motion with an accuracy of 6.57𝑚𝑚. We also validate the use of PPG data from OmniRing for heart rate monitoring. We believe the platform can enable exciting applications in fitness tracking, metaverse, sports, and healthcare.</i></p>
                </div>
            </td>
        </tr>









            
          <tr>
            <td width="33%" valign="top"><a href="pubs/sslotr.png"><img src="pubs/sslotr.png" width="100%" style="border-style: none"></a>
            <td width="67%" valign="top">
              <p><a href="https://dl.acm.org/doi/10.1145/3534587" id="">
              <heading>Learning on the Rings: Self-Supervised 3D Finger Motion Tracking using Wearable Sensors</heading></a><br>
              <strong>Hao Zhou</strong>, <FONT COLOR="#777777">Taiting Lu, Yilin Liu, Shijia Zhang, Mahanth Gowda</FONT>
              <br>
              <em>ACM IMWUT/UbiComp</em>, 2022.
              <br>
              <!--<em>International Conference on Computer Vision (ICCV)</em>, 2021 <strong style="color:red">(Oral)</strong>-->
              </p>

              <div class="paper" id="sslotr">
                
                <a href="javascript:toggleblock('sslotr_abs')">abstract</a> / 
                <!--<a shape="rect" href="javascript:togglebib('sslotr')" class="togglebib">bibtex</a> / -->
                <!-- <a href="https://raw.githubusercontent.com/hzhou3/hzhou3.github.io/main/bib/sslotr.bib">bibtex</a> / -->
                <a href="bib/sslotr.txt">bibtex</a> /
                <a href="slides/sslotr.pdf">slides</a>  
		      
                <p align="justify"> <i id="sslotr_abs">This paper presents ssLOTR (self-supervised learning on the rings), a system that shows the feasibility of designing self-supervised learning based techniques for 3D finger motion tracking using a custom-designed wearable inertial measurement unit (IMU) sensor with a minimal overhead of labeled training data. Ubiquitous finger motion tracking enables a number of applications in augmented and virtual reality, sign language recognition, rehabilitation healthcare, sports analytics, etc. However, unlike vision, there are no large-scale training datasets for developing robust machine learning (ML) models on wearable devices. ssLOTR designs ML models based on data augmentation and self-supervised learning to first extract efficient representations from raw IMU data without the need for any training labels. The extracted representations are further trained with small-scale labeled training data. In comparison to fully supervised learning, we show that only 15% of labeled training data is sufficient with self-supervised learning to achieve similar accuracy. Our sensor device is designed using a two-layer printed circuit board (PCB) to minimize the footprint and uses a combination of Polylactic acid (PLA) and Thermoplastic polyurethane (TPU) as housing materials for sturdiness and flexibility. It incorporates a system-on-chip (SoC) microcontroller with integrated WiFi/Bluetooth Low Energy (BLE) modules for real-time wireless communication, portability, and ubiquity. In contrast to gloves, our device is worn like rings on fingers, and therefore, does not impede dexterous finger motion. Extensive evaluation with 12 users depicts a 3D joint angle tracking accuracy of 9.07◦ (joint position accuracy of 6.55𝑚𝑚) with robustness to natural variation in sensor positions, wrist motion, etc, with low overhead in latency and power consumption on embedded platforms.</i></p>
  <!--
                <pre xml:space="preserve">
@Coming
  
                  </pre>
                -->
                </div>
            </td>
        </tr>




<!--<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">-->
            
          <tr>
            <td width="33%" valign="top"><a href="pubs/dachash.png"><img src="pubs/dachash.png" width="100%" style="border-style: none"></a>
            <td width="67%" valign="top">
              <p><a href="https://ieeexplore.ieee.org/document/9651569" id="">
              <heading>DACHash: A Dynamic, Cache-Aware and Concurrent Hash Table on GPUs</heading></a><br>
              <strong>Hao Zhou</strong>, <FONT COLOR="#777777">David Troendle, Byunghyun Jang</FONT>
              <br>
              <em>IEEE SBAC-PAD</em>, 2021. <strong style="color:red">(Best Paper Award)</strong>
              <br>
              <!--<em>International Conference on Computer Vision (ICCV)</em>, 2021 <strong style="color:red">(Oral)</strong>-->
              </p>

              <div class="paper" id="dachash">
                
                <a href="javascript:toggleblock('dachash_abs')">abstract</a> /
                <!--<a shape="rect" href="javascript:togglebib('dachash')" class="togglebib">bibtex</a>-->
                <!-- <a href="https://raw.githubusercontent.com/hzhou3/hzhou3.github.io/main/bib/dachash.bib">bibtex</a> -->
                <a href="bib/dachash.txt">bibtex</a>
          
                <p align="justify"> <i id="dachash_abs">
                GPU acceleration of hash tables in high-volume transaction applications such as computational geometry and bio-informatics are emerging. Recently, several hash table designs have been proposed on GPUs, but our analysis shows that they still do not adequately factor in several important aspects of a GPU’s execution environment, leaving large room for further optimization. To that end, we present a dynamic, cache-aware, concurrent hash table named DACHash. It is specifically designed to improve memory efficiency and reduce thread divergence on GPUs. We propose several novel techniques including a GPU-friendly data structure & sizing, a reorder algorithm, and dynamic thread-data mapping schemes that make the operations of hash table more amendable to GPU architecture. Testing DACHash on an NVIDIA GTX 3090 achieves a peak performance of 8.65 billion queries/second in static searching and 5.54 billion operations/second in concurrent operation execution. It outperforms the state-of-the-art SlabHash by 41.53% and 19.92% respectively. We also verify that our proposed technique improves L2 cache bandwidth and L2 cache hit rate by 9.18× and 2.68× respectively.</i></p>
  <!--
                <pre xml:space="preserve">
@inproceedings{zhou2021dachash,
  title = {DACHash: A Dynamic, Cache-Aware and Concurrent Hash Table on GPUs},
  author = {Zhou, Hao and Troendle, David and Jang, Byunghyun},
  booktitle = {2021 IEEE 33rd International Symposium on Computer Architecture and High Performance Computing (SBAC-PAD)},
  pages = {1--10},
  year = {2021},
  organization = {IEEE},
}
                  </pre>
                -->
                </div>
            </td>
        </tr>





<!--<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">-->
    <!--         
          <tr>
            <td width="33%" valign="top"><a href="pubs/oneclass.png"><img src="pubs/oneclass.png" width="100%" style="border-style: none"></a>
            <td width="67%" valign="top">
              <p><a href="https://arxiv.org/pdf/2204.09648.pdf" id="">
              <heading>One-Class Model for Fabric Defect Detection</heading></a><br>
              <strong>Hao Zhou</strong>, <FONT COLOR="#777777">Yixin Chen, David Troendle, Byunghyun Jang</FONT>
              <br>
              <em>MLTEC</em>, 2021.
              <br>
              </p>

              <div class="paper" id="oneclass">
                
                <a href="javascript:toggleblock('oneclass_abs')">abstract</a> /
                <a href="bib/oneclass.txt">bibtex</a> /
                
                <a href="https://github.com/hzhou3/one-class-dataset" >dataset</a> 
          
                <p align="justify"> <i id="oneclass_abs">
                An automated and accurate fabric defect inspection system is in high demand as a replacement for slow, inconsistent, error-prone, and expensive human operators in the textile industry. Previous efforts focused on certain types of fabrics or defects, which is not an ideal solution. In this paper, we propose a novel one-class model that is capable of detecting various defects on different fabric types. Our model takes advantage of a welldesigned Gabor filter bank to analyze fabric texture. We then leverage an advanced deep learning algorithm, autoencoder, to learn general feature representations from the outputs of the Gabor filter bank. Lastly, we develop a nearest neighbor density estimator to locate potential defects and draw them on the fabric images. We demonstrate the effectiveness and robustness of the proposed model by testing it on various types of fabrics such as plain, patterned, and rotated fabrics. Our model also achieves a true positive rate (a.k.a recall) value of 0.895 with no false alarms on our dataset based upon the Standard Fabric Defect Glossary.
                </i></p>
                </div>
            </td>
        </tr> -->







<!--<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">-->
            
          <tr>
            <td width="33%" valign="top"><a href="pubs/fasterfdd.png"><img src="pubs/fasterfdd.png" width="100%" style="border-style: none"></a>
            <td width="67%" valign="top">
              <p><a href="https://ieeexplore.ieee.org/document/9253108" id="">
              <heading>Exploring Faster RCNN for Fabric Defect Detection</heading></a><br>
              <strong>Hao Zhou</strong>, <FONT COLOR="#777777">Byunghyun Jang, Yixin Chen, David Troendle</FONT>
              <br>
              <em>IEEE AI4I</em>, 2020.
              <br>
              </p>

              <div class="paper" id="fasterfdd">
                
                <a href="javascript:toggleblock('fasterfdd_abs')">abstract</a> /
                <!--<a shape="rect" href="javascript:togglebib('fasterfdd')" class="togglebib">bibtex</a>-->
                <!-- <a href="https://raw.githubusercontent.com/hzhou3/hzhou3.github.io/main/bib/fasterfdd.bib">bibtex</a> -->
                <a href="bib/fasterfdd.txt">bibtex</a> /
                <a href="https://github.com/hzhou3/one-class-dataset" >related (dataset) </a> 
          
                <p align="justify"> <i id="fasterfdd_abs">
                This paper presents a fabric defect detection network (FabricNet) for automatic fabric defect detection. Our proposed FabricNet incorporates several effective techniques, such as Feature Pyramid Network (FPN), Deformable Convolution (DC) network, and Distance IoU Loss function, into vanilla Faster RCNN to improve the accuracy and speed of fabric defect detection. Our experiment shows that, when optimizations are combined, the FabricNet achieves 62.07% mAP and 97.37% AP50 on DAGM 2007 dataset, and an average prediction speed of 17 frames per second.
                </i></p>
  <!--
                <pre xml:space="preserve">
@inproceedings{zhou2020exploring,
  title={Exploring faster RCNN for fabric defect detection},
  author={Zhou, Hao and Jang, Byunghyun and Chen, Yixin and Troendle, David},
  booktitle={2020 Third International Conference on Artificial Intelligence for Industries (AI4I)},
  pages={52--55},
  year={2020},
  organization={IEEE},
}
                  </pre>
                -->
                </div>
            </td>
        </tr>
        </table>

      </div>




<hr class="soft">


















<!--services-->





    <button style="border:0px transparent; background-color: transparent;outline:none;"type="button" class="collapsible" data-toggle="collapse" data-target="#service" id="award_button"><h3>Services</h3></button>
    <div id="service" class="collapse in">
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr><td>
    
          </div>
          <ul>     
           <li><span> Invited Reviewer for Transactions on Mobile Computing</span></li>
           <li><span> Invited Reviewer for Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies</span></li>
           <li><span> Invited Reviewer for Journal of Intelligent Manufacturing</span></li>
           <li><span> Student Volunteer @ MobiQuitous '22</span></li>
          
                    </ul>

          
        </td></tr>
      </table>
      </div>


<hr class="soft">





























<!--awards-->



    <button style="border:0px transparent; background-color: transparent;outline:none;"type="button" class="collapsible" data-toggle="collapse" data-target="#award" id="award_button"><h3>Awards & Honors</h3></button>
    <div id="award" class="collapse in">
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr><td>     
          </div>
                    <ul>
                       <li><span> International Travel Grant, Penn State </span></li>	 
                       <li><span> Student Travel Grant, ACM MobiCom'23 </span></li>
                       <li><span> Best Paper Award for IoT Edge AI, IoTDI'23</span></li>
		                   <li><span> Outstanding TA Award, PennState</span></li>
                       <li><span> Best Paper Award, SBAC-PAD 2021</span></li>
                       <li><span> Summa Cum Laude, University of Mississippi </span></li>
                       <li><span> International Undergraduate Student Scholarship, University of Mississippi </span></li>
<!--                        <li><span> National Scholarship for Exchange Student, Beijing </span></li>
                       <li><span> National Scholarship, Ministry of Education of the People’s Republic of China </span></li>
                       <li><span> Outstanding Freshmen, North China University of Technology </span></li> -->
                      
                    </ul>


          
        </td></tr>
      </table>
      </div>
<hr class="soft">








<!--bottom-->

		<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
		<tr>
      <td>
		    <br>
        <p align="center"><font size="2">Last update on May 2024 and <a href="http://www.cs.berkeley.edu/~barron/">big shout out to Jon for the template</a></font></p>
      </td>
    </tr>
    </table>















<!--hide abs/bib at first-->

<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>

<script xml:space="preserve" language="JavaScript">
hideblock('earface_abs');
</script>

<script xml:space="preserve" language="JavaScript">
hideblock('signquery_abs');
</script>

<script xml:space="preserve" language="JavaScript">
hideblock('aslring_abs');
</script>


<script xml:space="preserve" language="JavaScript">
hideblock('omniring_abs');
</script>


<script xml:space="preserve" language="JavaScript">
hideblock('sslotr_abs');
</script>

<script xml:space="preserve" language="JavaScript">
hideblock('dachash_abs');
</script>
<!-- 
<script xml:space="preserve" language="JavaScript">
hideblock('oneclass_abs');
</script> -->

<script xml:space="preserve" language="JavaScript">
hideblock('fasterfdd_abs');
</script>




</body>
</html>
