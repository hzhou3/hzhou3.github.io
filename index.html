
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">

<html>
<head>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">



  <style type="text/css">
  a {
  color: #1772d0;
  text-decoration:none;
  }
  a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
  }
  body,td,th {
        font-family: 'Lato', Verdana, Helvetica, sans-serif;
        font-size: 14px
  }
  strong {
        font-family: 'Lato', Verdana, Helvetica, sans-serif;
        font-size: 14px
  }
  strongred {
        font-family: 'Lato', Verdana, Helvetica, sans-serif;
        color: 'red';
        font-size: 14px
  }
  heading {
        font-family: 'Lato', Verdana, Helvetica, sans-serif;
        font-size: 15px;
        font-weight: 700
  }



  </style>





  <link rel="icon" type="image/png" href="images/hao2.jpeg">
  <script type="text/javascript" src="js/hidebib.js"></script>




  <title>Hao Zhou</title>
  <meta name="Hao Zhou's Homepage"http-equiv="Content-Type" content="Hao Zhou's Homepage">

  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
  <script src='https://www.google.com/recaptcha/api.js'></script>


<script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]//function(){
    (i[r].q=i[r].q//[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-41102261-2', 'auto');
    ga('send', 'pageview');
</script>





</head>

<body>
  <table width="840" border="0" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <p align="center"><font size="6px">Hao Zhou Âë®Êµ©</font> <br>hfz5190@psu.edu</p>
            <tr>
            <td width="67%" valign="middle" align="justify">
              <p>
                I am a second-year Ph.D. student at PennState, advised by <a href="https://www.cse.psu.edu/~mkg31/">Prof. Mahanth Gowda</a>. Before starting my Ph.D. life, I recieved my M.S. and B.S. in Computer & Information Science at the University of Mississippi, advised by <a href="https://john.cs.olemiss.edu/~bjang/">Prof. Byunghyun Jang</a> and <a href="https://john.cs.olemiss.edu/~hcc/">Prof. H. Conrad Cunningham</a> respectively. 
              </p>
              <p>
                  I have board interests in Internet of Things (IoT), Applications of Machine Learning, GPGPU, Mobile and Wearable Computing.
              </p>

              <p style="text-align:center">
                <a href="cv/cv.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=Oc7AeVYAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/hzhou3">Github</a> &nbsp/&nbsp
                <a href="https://twitter.com/hao_zhh">Twitter</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/hao-zhou-37953316b">LinkedIn</a> 
              </p>
            </td>



            <td width="33%"valign="top"><img src="images/hao2-c.png" width="100%"></td>
          </tr>
        </table>



    <!--<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">-->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr><td>
            <h2>News</h2>
                    <ul>
                       <li><span> [2022-04] ssLOTR is accepted to IMWUT 2022.</span></li>
                       <li><span> [2021-10] DACHash won the <strong>Best Paper Award</strong>.</span></li>
                       <li><span> [2021-10] A paper is accepted to MLTEC 2021.</span></li>
                       <li><span> [2021-08] DACHash is accepted to SBAC-PAD 2021.</span></li>
                       <li><span> [2021-07] I moved to PennState!</span></li>
                       <li><span> [2020-08] My first paper is accepted to AI4I 2020.</span></li>
                      
                    </ul>
		    </td></tr>





    <!--<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">-->
    <!--<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">-->

        <tr><td>
            <h2>Publications</h2>
		  </td></tr>
    </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            
          <tr>
            <td width="33%" valign="top"><a href="pubs/sslotr.png"><img src="pubs/sslotr.png" width="100%" style="border-style: none"></a>
            <td width="67%" valign="top">
              <p><a href="https://www.google.com" id="">
              <heading>Learning on the Rings: Self-Supervised 3D Finger Motion Tracking using Wearable Sensors</heading></a><br>
              <strong>Hao Zhou</strong>*, Taiting Lu*, Yilin Liu, Shijia Zhang, Mahanth Gowda 
              <br>
              <em>ACM IMWUT/UbiComp</em>, 2022.
              <br>
              <!--<em>International Conference on Computer Vision (ICCV)</em>, 2021 <strong style="color:red">(Oral)</strong>-->
              </p>

              <div class="paper" id="sslotr">
                
                <a href="javascript:toggleblock('sslotr_abs')">abstract</a> / 
                <!--<a shape="rect" href="javascript:togglebib('sslotr')" class="togglebib">bibtex</a> / -->
                <a href="bib/sslotr.bib">bibtex</a> / 
                <a href="https://github.com/hzhou3/ssLOTR">code</a> 
		      
                <p align="justify"> <i id="sslotr_abs">This paper presents ssLOTR (self-supervised learning on the rings), a system that shows the feasibility of designing self-supervised learning based techniques for 3D finger motion tracking using a custom-designed wearable inertial measurement unit (IMU) sensor with a minimal overhead of labeled training data. Ubiquitous finger motion tracking enables a number of applications in augmented and virtual reality, sign language recognition, rehabilitation healthcare, sports analytics, etc. However, unlike vision, there are no large-scale training datasets for developing robust machine learning (ML) models on wearable devices. ssLOTR designs ML models based on data augmentation and self-supervised learning to first extract efficient representations from raw IMU data without the need for any training labels. The extracted representations are further trained with small-scale labeled training data. In comparison to fully supervised learning, we show that only 15% of labeled training data is sufficient with self-supervised learning to achieve similar accuracy. Our sensor device is designed using a two-layer printed circuit board (PCB) to minimize the footprint and uses a combination of Polylactic acid (PLA) and Thermoplastic polyurethane (TPU) as housing materials for sturdiness and flexibility. It incorporates a system-on-chip (SoC) microcontroller with integrated WiFi/Bluetooth Low Energy (BLE) modules for real-time wireless communication, portability, and ubiquity. In contrast to gloves, our device is worn like rings on fingers, and therefore, does not impede dexterous finger motion. Extensive evaluation with 12 users depicts a 3D joint angle tracking accuracy of 9.07‚ó¶ (joint position accuracy of 6.55ùëöùëö) with robustness to natural variation in sensor positions, wrist motion, etc, with low overhead in latency and power consumption on embedded platforms.</i></p>
  <!--
                <pre xml:space="preserve">
@Coming
  
                  </pre>
                -->
                </div>
            </td>
        </tr>




<!--<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">-->
            
          <tr>
            <td width="33%" valign="top"><a href="pubs/dachash.png"><img src="pubs/dachash.png" width="100%" style="border-style: none"></a>
            <td width="67%" valign="top">
              <p><a href="https://ieeexplore.ieee.org/document/9651569" id="">
              <heading>DACHash: A Dynamic, Cache-Aware and Concurrent Hash Table on GPUs</heading></a><br>
              <strong>Hao Zhou</strong>, David Troendle, Byunghyun Jang
              <br>
              <em>IEEE SBAC-PAD</em>, 2021. <strong style="color:red">(Best Paper Award)</strong>
              <br>
              <!--<em>International Conference on Computer Vision (ICCV)</em>, 2021 <strong style="color:red">(Oral)</strong>-->
              </p>

              <div class="paper" id="dachash">
                
                <a href="javascript:toggleblock('dachash_abs')">abstract</a> /
                <!--<a shape="rect" href="javascript:togglebib('dachash')" class="togglebib">bibtex</a>-->
                <a href="bib/dachash.bib">bibtex</a> 
          
                <p align="justify"> <i id="dachash_abs">
                GPU acceleration of hash tables in high-volume transaction applications such as computational geometry and bio-informatics are emerging. Recently, several hash table designs have been proposed on GPUs, but our analysis shows that they still do not adequately factor in several important aspects of a GPU‚Äôs execution environment, leaving large room for further optimization. To that end, we present a dynamic, cache-aware, concurrent hash table named DACHash. It is specifically designed to improve memory efficiency and reduce thread divergence on GPUs. We propose several novel techniques including a GPU-friendly data structure & sizing, a reorder algorithm, and dynamic thread-data mapping schemes that make the operations of hash table more amendable to GPU architecture. Testing DACHash on an NVIDIA GTX 3090 achieves a peak performance of 8.65 billion queries/second in static searching and 5.54 billion operations/second in concurrent operation execution. It outperforms the state-of-the-art SlabHash by 41.53% and 19.92% respectively. We also verify that our proposed technique improves L2 cache bandwidth and L2 cache hit rate by 9.18√ó and 2.68√ó respectively.</i></p>
  <!--
                <pre xml:space="preserve">
@inproceedings{zhou2021dachash,
title = {DACHash: A Dynamic, Cache-Aware and Concurrent Hash Table on GPUs},
author = {Zhou, Hao and Troendle, David and Jang, Byunghyun},
booktitle = {2021 IEEE 33rd International Symposium on Computer Architecture and High Performance Computing (SBAC-PAD)},
pages = {1--10},
year = {2021},
organization = {IEEE},
}
                  </pre>
                -->
                </div>
            </td>
        </tr>





<!--<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">-->
            
          <tr>
            <td width="33%" valign="top"><a href="pubs/oneclass.png"><img src="pubs/oneclass.png" width="100%" style="border-style: none"></a>
            <td width="67%" valign="top">
              <p><a href="https://arxiv.org/pdf/2204.09648.pdf" id="">
              <heading>One-Class Model for Fabric Defect Detection</heading></a><br>
              <strong>Hao Zhou</strong>, Yixin Chen, David Troendle, Byunghyun Jang
              <br>
              <em>MLTEC</em>, 2021.
              <br>
              </p>

              <div class="paper" id="oneclass">
                
                <a href="javascript:toggleblock('oneclass_abs')">abstract</a> /
                <!--<a shape="rect" href="javascript:togglebib('oneclass')" class="togglebib">bibtex</a> /-->
                <a href="bib/oneclass.bib">bibtex</a> /
                <a href="https://github.com/hzhou3/one-class-dataset">dataset</a> 
          
                <p align="justify"> <i id="oneclass_abs">
                An automated and accurate fabric defect inspection system is in high demand as a replacement for slow, inconsistent, error-prone, and expensive human operators in the textile industry. Previous efforts focused on certain types of fabrics or defects, which is not an ideal solution. In this paper, we propose a novel one-class model that is capable of detecting various defects on different fabric types. Our model takes advantage of a welldesigned Gabor filter bank to analyze fabric texture. We then leverage an advanced deep learning algorithm, autoencoder, to learn general feature representations from the outputs of the Gabor filter bank. Lastly, we develop a nearest neighbor density estimator to locate potential defects and draw them on the fabric images. We demonstrate the effectiveness and robustness of the proposed model by testing it on various types of fabrics such as plain, patterned, and rotated fabrics. Our model also achieves a true positive rate (a.k.a recall) value of 0.895 with no false alarms on our dataset based upon the Standard Fabric Defect Glossary.
                </i></p>
  <!--
                <pre xml:space="preserve">
@article{zhou2022one,
  title={One-Class Model for Fabric Defect Detection},
  author={Zhou, Hao and Chen, Yixin and Troendle, David and Jang, Byunghyun},
  journal={arXiv preprint arXiv:2204.09648},
  year={2022},
}
                  </pre>
                -->
                </div>
            </td>
        </tr>







<!--<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">-->
            
          <tr>
            <td width="33%" valign="top"><a href="pubs/fasterfdd.png"><img src="pubs/fasterfdd.png" width="100%" style="border-style: none"></a>
            <td width="67%" valign="top">
              <p><a href="https://ieeexplore.ieee.org/document/9253108" id="">
              <heading>Exploring Faster RCNN for Fabric Defect Detection</heading></a><br>
              <strong>Hao Zhou</strong>, Byunghyun Jang, Yixin Chen, David Troendle
              <br>
              <em>IEEE AI4I</em>, 2020.
              <br>
              </p>

              <div class="paper" id="fasterfdd">
                
                <a href="javascript:toggleblock('fasterfdd_abs')">abstract</a> /
                <!--<a shape="rect" href="javascript:togglebib('fasterfdd')" class="togglebib">bibtex</a>-->
                <a href="bib/fasterfdd.bib">bibtex</a>
          
                <p align="justify"> <i id="fasterfdd_abs">
                This paper presents a fabric defect detection network (FabricNet) for automatic fabric defect detection. Our proposed FabricNet incorporates several effective techniques, such as Feature Pyramid Network (FPN), Deformable Convolution (DC) network, and Distance IoU Loss function, into vanilla Faster RCNN to improve the accuracy and speed of fabric defect detection. Our experiment shows that, when optimizations are combined, the FabricNet achieves 62.07% mAP and 97.37% AP50 on DAGM 2007 dataset, and an average prediction speed of 17 frames per second.
                </i></p>
  <!--
                <pre xml:space="preserve">
@inproceedings{zhou2020exploring,
  title={Exploring faster RCNN for fabric defect detection},
  author={Zhou, Hao and Jang, Byunghyun and Chen, Yixin and Troendle, David},
  booktitle={2020 Third International Conference on Artificial Intelligence for Industries (AI4I)},
  pages={52--55},
  year={2020},
  organization={IEEE},
}
                  </pre>
                -->
                </div>
            </td>
        </tr>
        </table>


















    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr><td>
            <h2>Awards & Honors</h2>
                    <ul>
                       <li><span> Best Paper Award, SBAC-PAD 2021</span></li>
                       <li><span> Summa Cum Laude, University of Mississippi </span></li>
                       <li><span> International Undergraduate Student Scholarship, University of Mississippi </span></li>
                       <li><span> National Scholarship for Exchange Student, Beijing </span></li>
                       <li><span> National Scholarship, Ministry of Education of the People‚Äôs Republic of China </span></li>
                       <li><span> Outstanding Freshmen, North China University of Technology </span></li>
                      
                    </ul>
        </td></tr>
      </table>






		<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
		<tr><td>
		<br>
      <p align="right"><font size="2"><a href="http://www.cs.berkeley.edu/~barron/">Big shout out to Jon</a></font></p>
    </td></tr>
    </table>














<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>



<script xml:space="preserve" language="JavaScript">
hideblock('sslotr_abs');
</script>

<script xml:space="preserve" language="JavaScript">
hideblock('dachash_abs');
</script>

<script xml:space="preserve" language="JavaScript">
hideblock('oneclass_abs');
</script>

<script xml:space="preserve" language="JavaScript">
hideblock('fasterfdd_abs');
</script>



</body>
</html>
